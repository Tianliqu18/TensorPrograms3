{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOTNJ4zGs-hD",
        "outputId": "0226c683-b76b-495c-a9ba-4ad3a60e2ce4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- NETSOR Master Theorem Verification (n=5000, d=500) ---\n",
            "Sin/Cos Product:\n",
            "  Empirical (LHS): 0.00462\n",
            "  Theoretical (RHS): 0.00021\n",
            "  Error: 0.00441\n",
            "\n",
            "Non-linear Interaction:\n",
            "  Empirical (LHS): 0.40469\n",
            "  Theoretical (RHS): 0.39887\n",
            "  Error: 0.00582\n",
            "\n",
            "ReLU applied to all 5000x500 entries in the output matrix.\n",
            "Shape of activated matrix h: (5000, 500)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Setup dimensions\n",
        "n = 5000  # Number of coordinate slices (the \"width\")\n",
        "d = 500    # Number of input vectors (the \"input dimension\")\n",
        "\n",
        "# 2. Define Program Vectors\n",
        "# X consists of 500 input vectors, each of length n.\n",
        "# In the Master Theorem, these are vectors h^1...h^500\n",
        "X = np.random.normal(0, 1, size=(n, d))\n",
        "\n",
        "# W is the shared weight matrix (n x n)\n",
        "W_scaled_std = np.sqrt(1/n)\n",
        "W = np.random.normal(0, W_scaled_std, size=(n, n))\n",
        "\n",
        "# y is the output vector (n x d).\n",
        "# Every column y_j = W * x_j\n",
        "y = np.dot(W, X)\n",
        "\n",
        "# 3. Define Functions psi(x, y) to verify Theorem 2.10\n",
        "# We test a function on the first input vector and first output vector\n",
        "functions = {\n",
        "    \"Sin/Cos Product\": lambda x_vec, y_vec: np.sin(x_vec[:, 0]) * np.cos(y_vec[:, 0]),\n",
        "    \"Non-linear Interaction\": lambda x_vec, y_vec: np.maximum(0, x_vec[:, 0]) * y_vec[:, 0]**2,\n",
        "}\n",
        "\n",
        "print(f\"--- NETSOR Master Theorem Verification (n={n}, d={d}) ---\")\n",
        "\n",
        "# 4. Numerical Integration for Theoretical RHS (Monte Carlo)\n",
        "z_x = np.random.normal(0, 1, 1000000)\n",
        "z_y = np.random.normal(0, 1, 1000000)\n",
        "\n",
        "for name, psi in functions.items():\n",
        "    # LHS: Empirical average over n slices\n",
        "    # The function takes the full vectors and returns a value per slice\n",
        "    lhs = np.mean(psi(X, y))\n",
        "\n",
        "    # RHS: Gaussian expectation\n",
        "    rhs = np.mean(psi(z_x.reshape(-1, 1), z_y.reshape(-1, 1)))\n",
        "\n",
        "    print(f\"{name}:\")\n",
        "    print(f\"  Empirical (LHS): {lhs:.5f}\")\n",
        "    print(f\"  Theoretical (RHS): {rhs:.5f}\")\n",
        "    print(f\"  Error: {abs(lhs - rhs):.5f}\\n\")\n",
        "\n",
        "# --- APPLY RELU ---\n",
        "\n",
        "# 5. Apply ReLU coordinate-wise to the entire output matrix\n",
        "# As per Remark 2.11, Z^h is uniquely defined as ReLU(Z^y)\n",
        "h = np.maximum(0, y)\n",
        "\n",
        "print(f\"ReLU applied to all {n}x{d} entries in the output matrix.\")\n",
        "print(f\"Shape of activated matrix h: {h.shape}\")"
      ]
    }
  ]
}